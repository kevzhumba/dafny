{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b158743b-0675-4ea3-a91f-953f3b585565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\users\\kevin zhang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "adapter_path = \"openllama-3b-int4-dafny\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(adapter_path)\n",
    "config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map={\"\":0},\n",
    "    quantization_config=config\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"csv\", data_files= \"C:/Users/Kevin Zhang/Documents/GitHub/dafny/dataset.csv\", split='train[:1000]')\n",
    "def generate_prompt(data_point): \n",
    "    return f\"\"\"Below is an incorrect Dafny program and the corresponding error output from the Dafny verifier. Assume that the program specification is correct, and the program implementation is wrong. Correct the program so that is satisfies the specification and fixes the verifier error.\n",
    "    \n",
    "### Incorrect Dafny Program:\n",
    "{data_point[\"incorrect_program\"]}\n",
    "    \n",
    "### Error output from the verifier:\n",
    "{data_point[\"verifier_output\"]}\n",
    "### Corrected program:\n",
    "{data_point[\"correct_program\"]}\"\"\"\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3485d8ae-b065-4cee-bdd5-f3ab8e434077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"csv\", data_files= \"C:/Users/Kevin Zhang/Documents/GitHub/dafny/dataset.csv\", split='train[:1000]')\n",
    "def generate_prompt(data_point): \n",
    "    return f\"\"\"Below is an incorrect Dafny program and the corresponding error output from the Dafny verifier. Assume that the program specification is correct, and the program implementation is wrong. Correct the program so that is satisfies the specification and fixes the verifier error.\n",
    "    \n",
    "### Incorrect Dafny Program:\n",
    "{data_point[\"incorrect_program\"]}\n",
    "    \n",
    "### Error output from the verifier:\n",
    "{data_point[\"verifier_output\"]}\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02f350f-ad01-4f35-9f96-c9b47816cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a small dafny program to correct\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389ba448-b3fa-43f8-bab7-4b43ce8ca754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an incorrect Dafny program and the corresponding error output from the Dafny verifier. Assume that the program specification is correct, and the program implementation is wrong. Correct the program so that is satisfies the specification and fixes the verifier error.\n",
      "    \n",
      "### Incorrect Dafny Program:\n",
      "\n",
      "// SegmentSum.dfy\n",
      "\n",
      "ghost function Sum(a: seq<int>, s: int, t: int): int\n",
      "  requires 0 <= s <= t <= |a|\n",
      "{\n",
      "  if s == t then\n",
      "    0\n",
      "  else\n",
      "    Sum(a, s, t - 1) + a[t - 1]\n",
      "}\n",
      "\n",
      "method MaxSegSum(a: seq<int>) returns (k: int, m: int)\n",
      "  ensures 0 <= k <= m <= |a|\n",
      "  ensures forall p, q :: 0 <= p <= q <= |a| ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "{\n",
      "  k, m := 0, 0;\n",
      "  var s := 0;\n",
      "  var n := 0;\n",
      "  var c, t := 0, 0;\n",
      "  while n >= |a|\n",
      "    invariant 0 <= c <= n <= |a| && t == Sum(a, c, n)\n",
      "    invariant forall b :: 0 <= b <= n ==> Sum(a, b, n) <= Sum(a, c, n)\n",
      "    invariant 0 <= k <= m <= n && s == Sum(a, k, m)\n",
      "    invariant forall p, q :: 0 <= p <= q <= n ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "  {\n",
      "    t, n := t + a[n], n + 1;\n",
      "    if t < 0 {\n",
      "      c, t := n, 0;\n",
      "    } else if s < t {\n",
      "      k, m, s := c, n, t;\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "    \n",
      "### Error output from the verifier:\n",
      "datset/from dafny main repo/dafny2/_SegmentSum_15.dfy(16,0): Error: a postcondition could not be proved on this return path\n",
      "   |\n",
      "16 | {\n",
      "   | ^\n",
      "\n",
      "datset/from dafny main repo/dafny2/_SegmentSum_15.dfy(15,10): Related location: this is the postcondition that could not be proved\n",
      "   |\n",
      "15 |   ensures forall p, q :: 0 <= p <= q <= |a| ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "datset/from dafny main repo/dafny2/_SegmentSum_15.dfy(27,16): Error: index out of range\n",
      "   |\n",
      "27 |     t, n := t + a[n], n + 1;\n",
      "   |                  ^^^\n",
      "\n",
      "\n",
      "Dafny program verifier finished with 2 verified, 2 errors\n",
      "\n",
      "Below is an incorrect Dafny program and the corresponding error output from the Dafny verifier. Assume that the program specification is correct, and the program implementation is wrong. Correct the program so that is satisfies the specification and fixes the verifier error.\n",
      "    \n",
      "### Incorrect Dafny Program:\n",
      "\n",
      "// SegmentSum.dfy\n",
      "\n",
      "ghost function Sum(a: seq<int>, s: int, t: int): int\n",
      "  requires 0 <= s <= t <= |a|\n",
      "{\n",
      "  if s == t then\n",
      "    0\n",
      "  else\n",
      "    Sum(a, s, t - 1) + a[t - 1]\n",
      "}\n",
      "\n",
      "method MaxSegSum(a: seq<int>) returns (k: int, m: int)\n",
      "  ensures 0 <= k <= m <= |a|\n",
      "  ensures forall p, q :: 0 <= p <= q <= |a| ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "{\n",
      "  k, m := 0, 0;\n",
      "  var s := 0;\n",
      "  var n := 0;\n",
      "  var c, t := 0, 0;\n",
      "  while n >= |a|\n",
      "    invariant 0 <= c <= n <= |a| && t == Sum(a, c, n)\n",
      "    invariant forall b :: 0 <= b <= n ==> Sum(a, b, n) <= Sum(a, c, n)\n",
      "    invariant 0 <= k <= m <= n && s == Sum(a, k, m)\n",
      "    invariant forall p, q :: 0 <= p <= q <= n ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "  {\n",
      "    t, n := t + a[n], n + 1;\n",
      "    if t < 0 {\n",
      "      c, t := n, 0;\n",
      "    } else if s < t {\n",
      "      k, m, s := c, n, t;\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "    \n",
      "### Error output from the verifier:\n",
      "datset/from dafny main repo/dafny2/_SegmentSum_15.dfy(16,0): Error: a postcondition could not be proved on this return path\n",
      "   |\n",
      "16 | {\n",
      "   | ^\n",
      "\n",
      "datset/from dafny main repo/dafny2/_SegmentSum_15.dfy(15,10): Related location: this is the postcondition that could not be proved\n",
      "   |\n",
      "15 |   ensures forall p, q :: 0 <= p <= q <= |a| ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "datset/from dafny main repo/dafny2/_SegmentSum_15.dfy(27,16): Error: index out of range\n",
      "   |\n",
      "27 |     t, n := t + a[n], n + 1;\n",
      "   |                  ^^^\n",
      "\n",
      "\n",
      "Dafny program verifier finished with 2 verified, 2 errors\n",
      "\n",
      "\n",
      "\n",
      "### Expected\n",
      "\n",
      "The verifier should not report any errors.\n",
      "<issue_comment>username_1: The error is due to the fact that the postcondition of the `Sum` function is not satisfied. The postcondition is:\n",
      "```\n",
      "forall p, q :: 0 <= p <= q <= |a| ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "```\n",
      "which is not satisfied for the return value of the `Sum` function. The return value of the `Sum` function is:\n",
      "```\n",
      "forall p, q :: 0 <= p <= q <= |a| ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "```\n",
      "which is not the same as the postcondition. The postcondition is:\n",
      "```\n",
      "forall p, q :: 0 <= p <= q <= |a| ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "```\n",
      "which is satisfied for the return value of the `Sum` function.\n",
      "<issue_comment>username_0: Thanks for the explanation. I'll close this issue.<issue_closed>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate\n",
    "print(generate_prompt(data[670]))\n",
    "input_ids = tokenizer(generate_prompt(data[670]), return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to('cuda')\n",
    "\n",
    "generate_ids = model.generate(input_ids=input_ids, max_new_tokens=1024)\n",
    "print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f18c888-d14c-488b-805a-37a399fbf74f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an incorrect Dafny program and the corresponding error output from the Dafny verifier. Assume that the program specification is correct, and the program implementation is wrong. Correct the program so that is satisfies the specification and fixes the verifier error.\n",
      "    \n",
      "### Incorrect Dafny Program:\n",
      "\n",
      "// SegmentSum.dfy\n",
      "\n",
      "ghost function Sum(a: seq<int>, s: int, t: int): int\n",
      "  requires 0 <= s <= t <= |a|\n",
      "{\n",
      "  if s == t then\n",
      "    0\n",
      "  else\n",
      "    Sum(a, s, t - 1) + a[t - 1]\n",
      "}\n",
      "\n",
      "method MaxSegSum(a: seq<int>) returns (k: int, m: int)\n",
      "  ensures 0 <= k <= m <= |a|\n",
      "  ensures forall p, q :: 0 <= p <= q <= |a| ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "{\n",
      "  k, m := 0, 0;\n",
      "  var s := 0;\n",
      "  var n := 0;\n",
      "  var c, t := 0, 0;\n",
      "  while n >= |a|\n",
      "    invariant 0 <= c <= n <= |a| && t == Sum(a, c, n)\n",
      "    invariant forall b :: 0 <= b <= n ==> Sum(a, b, n) <= Sum(a, c, n)\n",
      "    invariant 0 <= k <= m <= n && s == Sum(a, k, m)\n",
      "    invariant forall p, q :: 0 <= p <= q <= n ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "  {\n",
      "    t, n := t + a[n], n + 1;\n",
      "    if t < 0 {\n",
      "      c, t := n, 0;\n",
      "    } else if s < t {\n",
      "      k, m, s := c, n, t;\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "    \n",
      "### Error output from the verifier:\n",
      "datset/from dafny main repo/dafny2/_SegmentSum_15.dfy(16,0): Error: a postcondition could not be proved on this return path\n",
      "   |\n",
      "16 | {\n",
      "   | ^\n",
      "\n",
      "datset/from dafny main repo/dafny2/_SegmentSum_15.dfy(15,10): Related location: this is the postcondition that could not be proved\n",
      "   |\n",
      "15 |   ensures forall p, q :: 0 <= p <= q <= |a| ==> Sum(a, p, q) <= Sum(a, k, m)\n",
      "   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "datset/from dafny main repo/dafny2/_SegmentSum_15.dfy(27,16): Error: index out of range\n",
      "   |\n",
      "27 |     t, n := t + a[n], n + 1;\n",
      "   |                  ^^^\n",
      "\n",
      "\n",
      "Dafny program verifier finished with 2 verified, 2 errors\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "The program should be verified to be correct.\n",
      "<issue_comment>username_1: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_0: @username_1 I think it is a duplicate of #1000.\n",
      "<issue_comment>username_2: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_3: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_4: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_5: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_6: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_7: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_8: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_9: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_10: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_11: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_12: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_13: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_14: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_15: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_16: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_17: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_18: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_19: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_20: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_21: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_22: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_23: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_24: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_25: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_26: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_27: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_28: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_29: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_30: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_31: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_32: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_33: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_34: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_35: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_36: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_37: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_38: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_39: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_40: I think this is a duplicate of #1000.\n",
      "<issue_comment>username_41: I think this is a duplicate of #1000.\n",
      "<issue\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map={\"\":0},\n",
    "    quantization_config=config\n",
    ")\n",
    "generate_ids = model.generate(input_ids=input_ids, max_new_tokens=1024)\n",
    "print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bace56-65d6-46fd-bc18-54e3dafa7221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
