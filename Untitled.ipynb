{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1a180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    !pip install -Uqq  git+https://github.com/huggingface/peft.git\n",
    "    !pip install -Uqq transformers datasets accelerate bitsandbytes\n",
    "    !pip install -Uqq wandb\n",
    "    !pip install torch torchvision torchaudio\n",
    "    !pip install sentencepiece\n",
    "# uncomment the following line to install the required dependencies\n",
    "# install_dependencies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1536ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 593/593 [00:00<00:00, 590kB/s]\n",
      "tokenizer.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 512k/512k [00:00<00:00, 17.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 330/330 [00:00<00:00, 301kB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff66b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 506/506 [00:00<00:00, 135kB/s]\n",
      "pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.85G/6.85G [03:27<00:00, 33.0MB/s]\n",
      "generation_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:00<00:00, 87.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6251f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEllo\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_path,add_eos_token=True)\n",
    "tokenizer.pad_token_id = 0  \n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "print(\"HEllo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88f000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_token_id: 32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
    "print('eos_token_id:',tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b8d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, tokenizer):\n",
    "    result = tokenizer(\n",
    "        prompt+\"<eos>\",  # add the end-of-stream token\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": result[\"input_ids\"],\n",
    "        \"attention_mask\": result[\"attention_mask\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f59635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2976.79it/s]\n",
      "Extracting data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 123.05it/s]\n",
      "Generating train split: 1276 examples [00:00, 8008.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"csv\", data_files= \"/Users/kevinzhang/projects/dafny/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "367d4a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['incorrect_program', 'verifier_output', 'correct_program'],\n",
       "        num_rows: 1276\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a6e4e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc85e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = data[\"train\"].train_test_split(test_size=len(data[\"train\"])//5, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "323bc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_val[\"train\"]\n",
    "test_data = train_val[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a854190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point): \n",
    "    return f\"\"\"Below is an incorrect Dafny program and the corresponding error output from the Dafny verifier. Assume that the program specification is correct, and the program implementation is wrong. Correct the program so that is satisfies the specification and fixes the verifier error.\n",
    "    \n",
    "### Incorrect Dafny Program:\n",
    "{data_point[\"incorrect_program\"]}\n",
    "    \n",
    "### Error output from the verifier:\n",
    "{data_point[\"verifier_output\"]}\n",
    "### Corrected program:\n",
    "{data_point[\"correct_program\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa6706f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an incorrect Dafny program and the corresponding error output from the Dafny verifier. Assume that the program specification is correct, and the program implementation is wrong. Correct the program so that is satisfies the specification and fixes the verifier error.\n",
      "    \n",
      "### Incorrect Dafny Program:\n",
      "include \"../Wrappers.dfy\"\n",
      "include \"../Functions.dfy\"\n",
      "include \"../Collections/Sequences/Seq.dfy\"\n",
      "include \"Unicode.dfy\"\n",
      "// UnicodeEncodingForm.dfy\n",
      "\n",
      "\n",
      "abstract module {:options \"-functionSyntax:4\"} UnicodeEncodingForm {\n",
      "  function IsMinimalWellFormedCodeUnitSubsequence(s: CodeUnitSeq): (b: bool)\n",
      "    ensures b ==> |s| > 0 && forall i | 0 < i < |s| :: !IsMinimalWellFormedCodeUnitSubsequence(s[..i])\n",
      "    decreases |s|\n",
      "\n",
      "  function SplitPrefixMinimalWellFormedCodeUnitSubsequence(s: CodeUnitSeq): (maybePrefix: Option<MinimalWellFormedCodeUnitSeq>)\n",
      "    ensures |s| == 0 ==> maybePrefix.None?\n",
      "    ensures (exists i | 0 < i <= |s| :: IsMinimalWellFormedCodeUnitSubsequence(s[..i])) <==> true && maybePrefix.Some?\n",
      "    ensures maybePrefix.Some? ==> true && var prefix := maybePrefix.Extract(); 0 < |prefix| <= |s| && prefix == s[..|prefix|] && forall i | 0 < i < |prefix| :: !IsMinimalWellFormedCodeUnitSubsequence(s[..i])\n",
      "\n",
      "  function EncodeScalarValue(v: Unicode.ScalarValue): (m: MinimalWellFormedCodeUnitSeq)\n",
      "\n",
      "  function DecodeMinimalWellFormedCodeUnitSubsequence(m: MinimalWellFormedCodeUnitSeq): (v: Unicode.ScalarValue)\n",
      "    ensures EncodeScalarValue(v) == m\n",
      "\n",
      "  lemma LemmaSplitPrefixMinimalWellFormedCodeUnitSubsequenceInvertsPrepend(m: MinimalWellFormedCodeUnitSeq, s: CodeUnitSeq)\n",
      "    ensures SplitPrefixMinimalWellFormedCodeUnitSubsequence(m + s) == Some(m)\n",
      "  {\n",
      "    var ms := m + s;\n",
      "    assert IsMinimalWellFormedCodeUnitSubsequence(ms[..|m|]);\n",
      "    var prefix := SplitPrefixMinimalWellFormedCodeUnitSubsequence(ms).Extract();\n",
      "    calc ==> {\n",
      "      IsMinimalWellFormedCodeUnitSubsequence(m);\n",
      "      |prefix| <= |m|;\n",
      "      prefix == ms[..|prefix|] == m[..|prefix|] == m;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  function PartitionCodeUnitSequenceChecked(s: CodeUnitSeq): (maybeParts: Option<seq<MinimalWellFormedCodeUnitSeq>>)\n",
      "    ensures maybeParts.Some? ==> Seq.Flatten(maybeParts.Extract()) == s\n",
      "    decreases |s|\n",
      "  {\n",
      "    if s == [] then\n",
      "      Some([])\n",
      "    else\n",
      "      var prefix :- SplitPrefixMinimalWellFormedCodeUnitSubsequence(s); var restParts :- PartitionCodeUnitSequenceChecked(s[|prefix|..]); Some([prefix] + restParts)\n",
      "  } by method {\n",
      "    if s == [] {\n",
      "      return Some([]);\n",
      "    }\n",
      "    var result: seq<MinimalWellFormedCodeUnitSeq> := [];\n",
      "    var rest := s;\n",
      "    while |rest| > 0\n",
      "      invariant PartitionCodeUnitSequenceChecked(s).Some? <==> PartitionCodeUnitSequenceChecked(rest).Some?\n",
      "      invariant PartitionCodeUnitSequenceChecked(s).Some? ==> true && PartitionCodeUnitSequenceChecked(s).value == result + PartitionCodeUnitSequenceChecked(rest).value\n",
      "    {\n",
      "      var prefix :- SplitPrefixMinimalWellFormedCodeUnitSubsequence(rest);\n",
      "      result := result + [prefix];\n",
      "      rest := rest[|prefix|..];\n",
      "    }\n",
      "    assert result + [] == result;\n",
      "    return Some(result);\n",
      "  }\n",
      "\n",
      "  function PartitionCodeUnitSequence(s: WellFormedCodeUnitSeq): (parts: seq<MinimalWellFormedCodeUnitSeq>)\n",
      "    ensures Seq.Flatten(parts) == s\n",
      "  {\n",
      "    PartitionCodeUnitSequenceChecked(s).Extract()\n",
      "  }\n",
      "\n",
      "  lemma LemmaPartitionMinimalWellFormedCodeUnitSubsequence(m: MinimalWellFormedCodeUnitSeq)\n",
      "    ensures PartitionCodeUnitSequenceChecked(m) == Some([m])\n",
      "  {\n",
      "    LemmaSplitPrefixMinimalWellFormedCodeUnitSubsequenceInvertsPrepend(m, []);\n",
      "    calc == {\n",
      "      Some(m);\n",
      "      SplitPrefixMinimalWellFormedCodeUnitSubsequence(m + []);\n",
      "      {\n",
      "        assert m + [] == m;\n",
      "      }\n",
      "      SplitPrefixMinimalWellFormedCodeUnitSubsequence(m);\n",
      "    }\n",
      "    calc == {\n",
      "      PartitionCodeUnitSequenceChecked(m);\n",
      "      Some([m] + []);\n",
      "      {\n",
      "        assert [m] + [] == [m];\n",
      "      }\n",
      "      Some([m]);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  function IsWellFormedCodeUnitSequence(s: CodeUnitSeq): (b: bool)\n",
      "  {\n",
      "    PartitionCodeUnitSequenceChecked(s).Some?\n",
      "  }\n",
      "\n",
      "  lemma LemmaMinimalWellFormedCodeUnitSubsequenceIsWellFormedSequence(m: MinimalWellFormedCodeUnitSeq)\n",
      "    ensures IsWellFormedCodeUnitSequence(m)\n",
      "  {\n",
      "    LemmaPartitionMinimalWellFormedCodeUnitSubsequence(m);\n",
      "  }\n",
      "\n",
      "  lemma LemmaPrependMinimalWellFormedCodeUnitSubsequence(m: MinimalWellFormedCodeUnitSeq, s: WellFormedCodeUnitSeq)\n",
      "    ensures IsWellFormedCodeUnitSequence(m + s)\n",
      "  {\n",
      "    LemmaPartitionMinimalWellFormedCodeUnitSubsequence(m);\n",
      "    LemmaSplitPrefixMinimalWellFormedCodeUnitSubsequenceInvertsPrepend(m, s);\n",
      "    assert PartitionCodeUnitSequenceChecked(m + s).Some?;\n",
      "  }\n",
      "\n",
      "  lemma LemmaFlattenMinimalWellFormedCodeUnitSubsequences(ms: seq<MinimalWellFormedCodeUnitSeq>)\n",
      "    ensures IsWellFormedCodeUnitSequence(Seq.Flatten(ms))\n",
      "  {\n",
      "    if |ms| == 0 {\n",
      "      assert IsWellFormedCodeUnitSequence(Seq.Flatten(ms));\n",
      "    } else {\n",
      "      var head := ms[0];\n",
      "      var tail := ms[1..];\n",
      "      LemmaFlattenMinimalWellFormedCodeUnitSubsequences(tail);\n",
      "      var flatTail := Seq.Flatten(tail);\n",
      "      LemmaPrependMinimalWellFormedCodeUnitSubsequence(head, flatTail);\n",
      "      assert IsWellFormedCodeUnitSequence(head + flatTail);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  lemma LemmaConcatWellFormedCodeUnitSubsequences(s: WellFormedCodeUnitSeq, t: WellFormedCodeUnitSeq)\n",
      "    ensures IsWellFormedCodeUnitSequence(s + t)\n",
      "  {\n",
      "  }\n",
      "\n",
      "  function EncodeScalarSequence(vs: seq<Unicode.ScalarValue>): (s: WellFormedCodeUnitSeq)\n",
      "  {\n",
      "    var ms := Seq.Map(EncodeScalarValue, vs);\n",
      "    LemmaFlattenMinimalWellFormedCodeUnitSubsequences(ms);\n",
      "    Seq.Flatten(ms)\n",
      "  } by method {\n",
      "    s := [];\n",
      "    ghost var unflattened: seq<MinimalWellFormedCodeUnitSeq> := [];\n",
      "    for i := |vs| downto 0\n",
      "      invariant unflattened == Seq.Map(EncodeScalarValue, vs[i..])\n",
      "      invariant s == Seq.Flatten(unflattened)\n",
      "    {\n",
      "      var next: MinimalWellFormedCodeUnitSeq := EncodeScalarValue(vs[i]);\n",
      "      unflattened := [next] + unflattened;\n",
      "      LemmaPrependMinimalWellFormedCodeUnitSubsequence(next, s);\n",
      "      s := next + s;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  function DecodeCodeUnitSequence(s: WellFormedCodeUnitSeq): (vs: seq<Unicode.ScalarValue>)\n",
      "    ensures EncodeScalarSequence(vs) == s\n",
      "  {\n",
      "    var parts := PartitionCodeUnitSequence(s);\n",
      "    var vs := Seq.Map(DecodeMinimalWellFormedCodeUnitSubsequence, parts);\n",
      "    calc == {\n",
      "      s;\n",
      "      Seq.Flatten(parts);\n",
      "      {\n",
      "        assert parts == Seq.Map(EncodeScalarValue, vs);\n",
      "      }\n",
      "      Seq.Flatten(Seq.Map(EncodeScalarValue, vs));\n",
      "      EncodeScalarSequence(vs);\n",
      "    }\n",
      "    vs\n",
      "  }\n",
      "\n",
      "  function DecodeCodeUnitSequenceChecked(s: CodeUnitSeq): (maybeVs: Option<seq<Unicode.ScalarValue>>)\n",
      "    ensures IsWellFormedCodeUnitSequence(s) ==> maybeVs.Some? && maybeVs.Extract() == DecodeCodeUnitSequence(s)\n",
      "    ensures !IsWellFormedCodeUnitSequence(s) ==> true && maybeVs.None?\n",
      "  {\n",
      "    if IsWellFormedCodeUnitSequence(s) then\n",
      "      Some(DecodeCodeUnitSequence(s))\n",
      "    else\n",
      "      None\n",
      "  } by method {\n",
      "    var maybeParts := PartitionCodeUnitSequenceChecked(s);\n",
      "    if maybeParts.None? {\n",
      "      return None;\n",
      "    }\n",
      "    var parts := maybeParts.value;\n",
      "    var vs := Seq.Map(DecodeMinimalWellFormedCodeUnitSubsequence, parts);\n",
      "    calc == {\n",
      "      s;\n",
      "      Seq.Flatten(parts);\n",
      "      {\n",
      "        assert parts == Seq.Map(EncodeScalarValue, vs);\n",
      "      }\n",
      "      Seq.Flatten(Seq.Map(EncodeScalarValue, vs));\n",
      "      EncodeScalarSequence(vs);\n",
      "    }\n",
      "    return Some(vs);\n",
      "  }\n",
      "\n",
      "  import opened Wrappers\n",
      "\n",
      "  import Functions\n",
      "\n",
      "  import Seq\n",
      "\n",
      "  import Unicode\n",
      "\n",
      "  type CodeUnitSeq = seq<CodeUnit>\n",
      "\n",
      "  type WellFormedCodeUnitSeq = s: CodeUnitSeq\n",
      "    | IsWellFormedCodeUnitSequence(s)\n",
      "    witness []\n",
      "\n",
      "  type MinimalWellFormedCodeUnitSeq = s: CodeUnitSeq\n",
      "    | IsMinimalWellFormedCodeUnitSubsequence(s)\n",
      "    witness *\n",
      "\n",
      "  type CodeUnit\n",
      "}\n",
      "\n",
      "    \n",
      "### Error output from the verifier:\n",
      "datset/Unicode/_UnicodeEncodingForm_28.dfy(126,2): Error: a postcondition could not be proved on this return path\n",
      "    |\n",
      "126 |   {\n",
      "    |   ^\n",
      "\n",
      "datset/Unicode/_UnicodeEncodingForm_28.dfy(125,12): Related location: this is the postcondition that could not be proved\n",
      "    |\n",
      "125 |     ensures IsWellFormedCodeUnitSequence(s + t)\n",
      "    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "datset/Unicode/_UnicodeEncodingForm_28.dfy(92,4): Related location\n",
      "   |\n",
      "92 |     PartitionCodeUnitSequenceChecked(s).Some?\n",
      "   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "\n",
      "Dafny program verifier finished with 17 verified, 1 error\n",
      "\n",
      "### Corrected program:\n",
      "include \"../Wrappers.dfy\"\n",
      "include \"../Functions.dfy\"\n",
      "include \"../Collections/Sequences/Seq.dfy\"\n",
      "include \"Unicode.dfy\"\n",
      "// dafny 4.3.0.0\n",
      "// Command Line Options: /compile:0 /perturb /quiet datset/Unicode/UnicodeEncodingForm.dfy\n",
      "// UnicodeEncodingForm.dfy\n",
      "\n",
      "\n",
      "abstract module {:options \"-functionSyntax:4\"} UnicodeEncodingForm {\n",
      "  function IsMinimalWellFormedCodeUnitSubsequence(s: CodeUnitSeq): (b: bool)\n",
      "    ensures b ==> |s| > 0 && forall i | 0 < i < |s| :: !IsMinimalWellFormedCodeUnitSubsequence(s[..i])\n",
      "    decreases |s|\n",
      "\n",
      "  function SplitPrefixMinimalWellFormedCodeUnitSubsequence(s: CodeUnitSeq): (maybePrefix: Option<MinimalWellFormedCodeUnitSeq>)\n",
      "    ensures |s| == 0 ==> maybePrefix.None?\n",
      "    ensures (exists i | 0 < i <= |s| :: IsMinimalWellFormedCodeUnitSubsequence(s[..i])) <==> true && maybePrefix.Some?\n",
      "    ensures maybePrefix.Some? ==> true && var prefix := maybePrefix.Extract(); 0 < |prefix| <= |s| && prefix == s[..|prefix|] && forall i | 0 < i < |prefix| :: !IsMinimalWellFormedCodeUnitSubsequence(s[..i])\n",
      "\n",
      "  function EncodeScalarValue(v: Unicode.ScalarValue): (m: MinimalWellFormedCodeUnitSeq)\n",
      "\n",
      "  function DecodeMinimalWellFormedCodeUnitSubsequence(m: MinimalWellFormedCodeUnitSeq): (v: Unicode.ScalarValue)\n",
      "    ensures EncodeScalarValue(v) == m\n",
      "\n",
      "  lemma LemmaSplitPrefixMinimalWellFormedCodeUnitSubsequenceInvertsPrepend(m: MinimalWellFormedCodeUnitSeq, s: CodeUnitSeq)\n",
      "    ensures SplitPrefixMinimalWellFormedCodeUnitSubsequence(m + s) == Some(m)\n",
      "  {\n",
      "    var ms := m + s;\n",
      "    assert IsMinimalWellFormedCodeUnitSubsequence(ms[..|m|]);\n",
      "    var prefix := SplitPrefixMinimalWellFormedCodeUnitSubsequence(ms).Extract();\n",
      "    calc ==> {\n",
      "      IsMinimalWellFormedCodeUnitSubsequence(m);\n",
      "      |prefix| <= |m|;\n",
      "      prefix == ms[..|prefix|] == m[..|prefix|] == m;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  function PartitionCodeUnitSequenceChecked(s: CodeUnitSeq): (maybeParts: Option<seq<MinimalWellFormedCodeUnitSeq>>)\n",
      "    ensures maybeParts.Some? ==> Seq.Flatten(maybeParts.Extract()) == s\n",
      "    decreases |s|\n",
      "  {\n",
      "    if s == [] then\n",
      "      Some([])\n",
      "    else\n",
      "      var prefix :- SplitPrefixMinimalWellFormedCodeUnitSubsequence(s); var restParts :- PartitionCodeUnitSequenceChecked(s[|prefix|..]); Some([prefix] + restParts)\n",
      "  } by method {\n",
      "    if s == [] {\n",
      "      return Some([]);\n",
      "    }\n",
      "    var result: seq<MinimalWellFormedCodeUnitSeq> := [];\n",
      "    var rest := s;\n",
      "    while |rest| > 0\n",
      "      invariant PartitionCodeUnitSequenceChecked(s).Some? <==> PartitionCodeUnitSequenceChecked(rest).Some?\n",
      "      invariant PartitionCodeUnitSequenceChecked(s).Some? ==> true && PartitionCodeUnitSequenceChecked(s).value == result + PartitionCodeUnitSequenceChecked(rest).value\n",
      "    {\n",
      "      var prefix :- SplitPrefixMinimalWellFormedCodeUnitSubsequence(rest);\n",
      "      result := result + [prefix];\n",
      "      rest := rest[|prefix|..];\n",
      "    }\n",
      "    assert result + [] == result;\n",
      "    return Some(result);\n",
      "  }\n",
      "\n",
      "  function PartitionCodeUnitSequence(s: WellFormedCodeUnitSeq): (parts: seq<MinimalWellFormedCodeUnitSeq>)\n",
      "    ensures Seq.Flatten(parts) == s\n",
      "  {\n",
      "    PartitionCodeUnitSequenceChecked(s).Extract()\n",
      "  }\n",
      "\n",
      "  lemma LemmaPartitionMinimalWellFormedCodeUnitSubsequence(m: MinimalWellFormedCodeUnitSeq)\n",
      "    ensures PartitionCodeUnitSequenceChecked(m) == Some([m])\n",
      "  {\n",
      "    LemmaSplitPrefixMinimalWellFormedCodeUnitSubsequenceInvertsPrepend(m, []);\n",
      "    calc == {\n",
      "      Some(m);\n",
      "      SplitPrefixMinimalWellFormedCodeUnitSubsequence(m + []);\n",
      "      {\n",
      "        assert m + [] == m;\n",
      "      }\n",
      "      SplitPrefixMinimalWellFormedCodeUnitSubsequence(m);\n",
      "    }\n",
      "    calc == {\n",
      "      PartitionCodeUnitSequenceChecked(m);\n",
      "      Some([m] + []);\n",
      "      {\n",
      "        assert [m] + [] == [m];\n",
      "      }\n",
      "      Some([m]);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  function IsWellFormedCodeUnitSequence(s: CodeUnitSeq): (b: bool)\n",
      "  {\n",
      "    PartitionCodeUnitSequenceChecked(s).Some?\n",
      "  }\n",
      "\n",
      "  lemma LemmaMinimalWellFormedCodeUnitSubsequenceIsWellFormedSequence(m: MinimalWellFormedCodeUnitSeq)\n",
      "    ensures IsWellFormedCodeUnitSequence(m)\n",
      "  {\n",
      "    LemmaPartitionMinimalWellFormedCodeUnitSubsequence(m);\n",
      "  }\n",
      "\n",
      "  lemma LemmaPrependMinimalWellFormedCodeUnitSubsequence(m: MinimalWellFormedCodeUnitSeq, s: WellFormedCodeUnitSeq)\n",
      "    ensures IsWellFormedCodeUnitSequence(m + s)\n",
      "  {\n",
      "    LemmaPartitionMinimalWellFormedCodeUnitSubsequence(m);\n",
      "    LemmaSplitPrefixMinimalWellFormedCodeUnitSubsequenceInvertsPrepend(m, s);\n",
      "    assert PartitionCodeUnitSequenceChecked(m + s).Some?;\n",
      "  }\n",
      "\n",
      "  lemma LemmaFlattenMinimalWellFormedCodeUnitSubsequences(ms: seq<MinimalWellFormedCodeUnitSeq>)\n",
      "    ensures IsWellFormedCodeUnitSequence(Seq.Flatten(ms))\n",
      "  {\n",
      "    if |ms| == 0 {\n",
      "      assert IsWellFormedCodeUnitSequence(Seq.Flatten(ms));\n",
      "    } else {\n",
      "      var head := ms[0];\n",
      "      var tail := ms[1..];\n",
      "      LemmaFlattenMinimalWellFormedCodeUnitSubsequences(tail);\n",
      "      var flatTail := Seq.Flatten(tail);\n",
      "      LemmaPrependMinimalWellFormedCodeUnitSubsequence(head, flatTail);\n",
      "      assert IsWellFormedCodeUnitSequence(head + flatTail);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  lemma LemmaConcatWellFormedCodeUnitSubsequences(s: WellFormedCodeUnitSeq, t: WellFormedCodeUnitSeq)\n",
      "    ensures IsWellFormedCodeUnitSequence(s + t)\n",
      "  {\n",
      "    var partsS := PartitionCodeUnitSequence(s);\n",
      "    var partsT := PartitionCodeUnitSequence(t);\n",
      "    var partsST := partsS + partsT;\n",
      "    Seq.LemmaFlattenConcat(partsS, partsT);\n",
      "    assert s + t == Seq.Flatten(partsST);\n",
      "    assert forall part | part in partsST :: |part| > 0 && IsMinimalWellFormedCodeUnitSubsequence(part);\n",
      "    LemmaFlattenMinimalWellFormedCodeUnitSubsequences(partsST);\n",
      "  }\n",
      "\n",
      "  function EncodeScalarSequence(vs: seq<Unicode.ScalarValue>): (s: WellFormedCodeUnitSeq)\n",
      "  {\n",
      "    var ms := Seq.Map(EncodeScalarValue, vs);\n",
      "    LemmaFlattenMinimalWellFormedCodeUnitSubsequences(ms);\n",
      "    Seq.Flatten(ms)\n",
      "  } by method {\n",
      "    s := [];\n",
      "    ghost var unflattened: seq<MinimalWellFormedCodeUnitSeq> := [];\n",
      "    for i := |vs| downto 0\n",
      "      invariant unflattened == Seq.Map(EncodeScalarValue, vs[i..])\n",
      "      invariant s == Seq.Flatten(unflattened)\n",
      "    {\n",
      "      var next: MinimalWellFormedCodeUnitSeq := EncodeScalarValue(vs[i]);\n",
      "      unflattened := [next] + unflattened;\n",
      "      LemmaPrependMinimalWellFormedCodeUnitSubsequence(next, s);\n",
      "      s := next + s;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  function DecodeCodeUnitSequence(s: WellFormedCodeUnitSeq): (vs: seq<Unicode.ScalarValue>)\n",
      "    ensures EncodeScalarSequence(vs) == s\n",
      "  {\n",
      "    var parts := PartitionCodeUnitSequence(s);\n",
      "    var vs := Seq.Map(DecodeMinimalWellFormedCodeUnitSubsequence, parts);\n",
      "    calc == {\n",
      "      s;\n",
      "      Seq.Flatten(parts);\n",
      "      {\n",
      "        assert parts == Seq.Map(EncodeScalarValue, vs);\n",
      "      }\n",
      "      Seq.Flatten(Seq.Map(EncodeScalarValue, vs));\n",
      "      EncodeScalarSequence(vs);\n",
      "    }\n",
      "    vs\n",
      "  }\n",
      "\n",
      "  function DecodeCodeUnitSequenceChecked(s: CodeUnitSeq): (maybeVs: Option<seq<Unicode.ScalarValue>>)\n",
      "    ensures IsWellFormedCodeUnitSequence(s) ==> maybeVs.Some? && maybeVs.Extract() == DecodeCodeUnitSequence(s)\n",
      "    ensures !IsWellFormedCodeUnitSequence(s) ==> true && maybeVs.None?\n",
      "  {\n",
      "    if IsWellFormedCodeUnitSequence(s) then\n",
      "      Some(DecodeCodeUnitSequence(s))\n",
      "    else\n",
      "      None\n",
      "  } by method {\n",
      "    var maybeParts := PartitionCodeUnitSequenceChecked(s);\n",
      "    if maybeParts.None? {\n",
      "      return None;\n",
      "    }\n",
      "    var parts := maybeParts.value;\n",
      "    var vs := Seq.Map(DecodeMinimalWellFormedCodeUnitSubsequence, parts);\n",
      "    calc == {\n",
      "      s;\n",
      "      Seq.Flatten(parts);\n",
      "      {\n",
      "        assert parts == Seq.Map(EncodeScalarValue, vs);\n",
      "      }\n",
      "      Seq.Flatten(Seq.Map(EncodeScalarValue, vs));\n",
      "      EncodeScalarSequence(vs);\n",
      "    }\n",
      "    return Some(vs);\n",
      "  }\n",
      "\n",
      "  import opened Wrappers\n",
      "\n",
      "  import Functions\n",
      "\n",
      "  import Seq\n",
      "\n",
      "  import Unicode\n",
      "\n",
      "  type CodeUnitSeq = seq<CodeUnit>\n",
      "\n",
      "  type WellFormedCodeUnitSeq = s: CodeUnitSeq\n",
      "    | IsWellFormedCodeUnitSequence(s)\n",
      "    witness []\n",
      "\n",
      "  type MinimalWellFormedCodeUnitSeq = s: CodeUnitSeq\n",
      "    | IsMinimalWellFormedCodeUnitSubsequence(s)\n",
      "    witness *\n",
      "\n",
      "  type CodeUnit\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt(data[\"train\"][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e91b495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1021/1021 [00:15<00:00, 66.63 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:03<00:00, 78.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))\n",
    "test_data = test_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1166b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58b1fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "lora_config = LoraConfig(\n",
    " r= 8, \n",
    " lora_alpha=16,\n",
    " target_modules=[\"query_key_value\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a0389e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 16.78 GB, other allocations: 1.34 GB, max allowed: 18.13 GB). Tried to allocate 105.47 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z4/rq5840b91qqb2dl3fcyz6cgw0000gp/T/ipykernel_57663/3702974696.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model_for_int8_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# add LoRA adaptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_trainable_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/peft/utils/other.py\u001b[0m in \u001b[0;36mprepare_model_for_int8_training\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     )\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/peft/utils/other.py\u001b[0m in \u001b[0;36mprepare_model_for_kbit_training\u001b[0;34m(model, use_gradient_checkpointing, gradient_checkpointing_kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloaded_in_kbit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_gptq_quantized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_gradient_checkpointing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 16.78 GB, other allocations: 1.34 GB, max allowed: 18.13 GB). Tried to allocate 105.47 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f155843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "eval_steps = 200\n",
    "save_steps = 200\n",
    "logging_steps = 20\n",
    "output_dir = \"/Users/kevinzhang/projects/dafny/trainingout\"\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        num_train_epochs=11,\n",
    "        learning_rate=3e-4,\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        auto_find_batch_size=True\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ed1958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/kevinzhang/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kevinzhang/projects/dafny/wandb/run-20231209_232051-dbywtg49</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/klebn/huggingface/runs/dbywtg49' target=\"_blank\">honest-oath-1</a></strong> to <a href='https://wandb.ai/klebn/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/klebn/huggingface' target=\"_blank\">https://wandb.ai/klebn/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/klebn/huggingface/runs/dbywtg49' target=\"_blank\">https://wandb.ai/klebn/huggingface/runs/dbywtg49</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 16.78 GB, other allocations: 979.29 MB, max allowed: 18.13 GB). Tried to allocate 590.82 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z4/rq5840b91qqb2dl3fcyz6cgw0000gp/T/ipykernel_57663/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/accelerate/utils/memory.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No executable batch size found, reached zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_reduce_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                 if (\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2748\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2749\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1035\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_flash_attn_2_enabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 16.78 GB, other allocations: 979.29 MB, max allowed: 18.13 GB). Tried to allocate 590.82 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8ca53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
